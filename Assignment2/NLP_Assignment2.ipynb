{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import math\n",
        "nltk.download('punkt')  # Download necessary resources (if not already downloaded)\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "class SpellChecker:\n",
        "  def __init__(self, WORDS_FREQS, BIGRAMS, edit_distance=2, beam_search_N=20):\n",
        "    self.WORDS_FREQS = WORDS_FREQS\n",
        "    self.N = sum(WORDS_FREQS.values())\n",
        "    self.N_bigrams = sum(BIGRAMS.values())\n",
        "    self.BIGRAMS = BIGRAMS\n",
        "    self.edit_distance = edit_distance\n",
        "    self.beam_search_N = beam_search_N\n",
        "\n",
        "  def p(self, word):\n",
        "    if word.lower() not in self.WORDS_FREQS:\n",
        "      return 1\n",
        "    return self.WORDS_FREQS[word.lower()] / self.N\n",
        "\n",
        "  def two_word_seq_prob(self, word1, word2):\n",
        "    if (word1, word2) in self.BIGRAMS:\n",
        "      return self.BIGRAMS[(word1, word2)] / self.N_bigrams\n",
        "    else:\n",
        "      return self.p(word1) * self.p(word2)\n",
        "\n",
        "  def existed_word_neighbors(self, word):\n",
        "    existed_words = self.known([word.lower()])\n",
        "    current_edit_words = set([word.lower()])\n",
        "    for i in range(self.edit_distance):\n",
        "      current_words = set()\n",
        "      for word in current_edit_words:\n",
        "        current_words |= self.edits1(word)\n",
        "      existed_words += sorted(self.known(current_words), key=lambda x: -self.p(x))\n",
        "      current_edit_words = current_words\n",
        "    uniq_words = set()\n",
        "    ans = []\n",
        "    for word in existed_words:\n",
        "      if word not in uniq_words:\n",
        "        uniq_words.add(word)\n",
        "        ans.append(word)\n",
        "        if len(ans) == self.beam_search_N:\n",
        "          break\n",
        "    return ans\n",
        "  def edits1(self, word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "  def known(self, words):\n",
        "     return [w for w in words if w in self.WORDS_FREQS]\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    return [word_tokenize(sentence) for sentence in sent_tokenize(text)]\n",
        "  def prob_lighting(self, p):\n",
        "\n",
        "    return 2 / (1 + math.exp(-10 * p)) - 1\n",
        "  def sentence_correction(self, sentence):\n",
        "    if len(sentence) < 3: # One word sentence\n",
        "      candidates = self.existed_word_neighbors(sentence[0])+[sentence[0]]\n",
        "      if len(sentence) == 2:\n",
        "        return [candidates[0], sentence[-1]]\n",
        "      return [candidates[0]]\n",
        "    cur_idx = 1\n",
        "    first_candidates = self.existed_word_neighbors(sentence[0].lower())\n",
        "    if len(first_candidates) == 0:\n",
        "      first_candidates.append(sentence[0].lower())\n",
        "    beam_sentences = [[self.p(word) if word!=sentence[0].lower() else 1, [word]] for idx, word in enumerate(first_candidates)]\n",
        "    while cur_idx < len(sentence):\n",
        "      candidates = self.existed_word_neighbors(sentence[cur_idx].lower())\n",
        "      if len(candidates) == 0:\n",
        "        candidates.append(sentence[cur_idx].lower())\n",
        "      if not sentence[cur_idx].isalpha() or not sentence[cur_idx - 1].isalpha() or sentence[cur_idx].lower() in self.WORDS_FREQS or sentence[cur_idx - 1].lower() not in self.WORDS_FREQS or len(candidates) <= 1:\n",
        "        for beam_sentence in beam_sentences:\n",
        "          beam_sentence[1].append(sentence[cur_idx].lower())\n",
        "        cur_idx += 1\n",
        "        continue\n",
        "\n",
        "      longer_beam_sentences = []\n",
        "\n",
        "      for cand_idx, candidate in enumerate(candidates):\n",
        "        for beam_sentence in beam_sentences:\n",
        "          two_words_p = self.two_word_seq_prob(beam_sentence[1][-1].lower(), candidate.lower())\n",
        "          longer_beam_sentences.append([beam_sentence[0] * self.p(candidate) * self.prob_lighting(two_words_p), beam_sentence[1]+[candidate]])\n",
        "      # Normalizing probabilities\n",
        "      s = 0\n",
        "      for p, snt in longer_beam_sentences:\n",
        "        s += p\n",
        "      for i in range(len(longer_beam_sentences)):\n",
        "        longer_beam_sentences[i][0] /= s\n",
        "\n",
        "      beam_sentences = sorted(longer_beam_sentences, reverse=True)[:self.beam_search_N]\n",
        "      cur_idx+=1\n",
        "    return beam_sentences[0][1]\n",
        "\n",
        "  def text_correction(self, text):\n",
        "    sentences = self.tokenize(text)\n",
        "    ans = ''\n",
        "    for sentence in sentences:\n",
        "        ans += ' '.join(self.sentence_correction(sentence))+' '\n",
        "    return ans.strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ9El9i1jTel",
        "outputId": "5c39a7ac-7282-4290-e84d-3a3dc63f3bdf"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class NorvigSpellChecker:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.WORDS = Counter(self.words(open('big.txt').read()))\n",
        "\n",
        "  def words(self, text):\n",
        "     return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "  def P(self, word):\n",
        "    N=sum(self.WORDS.values())\n",
        "    return self.WORDS[word] / N\n",
        "\n",
        "  def correction(self, word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(self.candidates(word), key=self.P)\n",
        "\n",
        "  def candidates(self, word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
        "\n",
        "  def known(self, words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in self.WORDS)\n",
        "\n",
        "  def edits1(self, word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "  def edits2(self, word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "\n",
        "  def text_correction(self, text):\n",
        "    tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(text.lower())]\n",
        "    ans = ''\n",
        "    for sentence in tokenized_sentences:\n",
        "      for word in sentence:\n",
        "        nw = word\n",
        "        if word.isalpha():\n",
        "          nw = self.correction(word)\n",
        "        ans += nw + ' '\n",
        "    return ans"
      ],
      "metadata": {
        "id": "rYpVw33h0Pws"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"unigram_freq.csv\") # https://www.kaggle.com/datasets/rtatman/english-word-frequency\n",
        "df = df.dropna(axis=0)\n",
        "WORDS = {}\n",
        "for _, (word, count) in df.iloc[:10000].iterrows():\n",
        "  WORDS[word.lower()] = count\n",
        "print(WORDS['the'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwA5mAU2y_Qf",
        "outputId": "8b278567-711e-435e-8028-ea2e2533fa4c"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23135851162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = pd.read_csv(\"bigrams.txt\", delimiter='\\t', header=None, encoding='latin1')\n",
        "f = f.dropna(axis=0)\n",
        "BIGRAMS = {}\n",
        "for _, (count, word1, word2) in f.iterrows():\n",
        "  BIGRAMS[(word1.lower(), word2.lower())] = count\n",
        "print(BIGRAMS[('i', 'want')])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnhU-f6tznMB",
        "outputId": "14d1ab70-111f-4ec8-b3ce-5256208225b6"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "# Abstract\n",
        "I chose Norvig's solution as the baseline model, but my model takes into account bigrams and utilizes beam search to maximize the probabilities of correction. My algorithm incorporates a simple notion: if a word is present in the dataset, then I do not attempt to correct it.\n",
        "\n",
        "# Tokenization\n",
        "I divide the text into sentences and then further segment these sentences into words using the `nltk` library.\n",
        "\n",
        "# Word Probability\n",
        "I obtain word counts from `unigram_freq.csv`, a dataset sourced from Kaggle ([link](https://www.kaggle.com/datasets/rtatman/english-word-frequency)). I utilize only the first 40,000 rows (the 40,000 most popular words) to slightly expedite the algorithm and exclude unfamiliar words such as 'evh' or 'sqv'. Although I am uncertain as to why these words are present in the dataset, the algorithm performs significantly better without them.\n",
        "\n",
        "# Candidate Selection\n",
        "I employ Norvig's solution, albeit with additional options. While Norvig's solution typically considers words within a two-edit distance, my algorithm includes an `edit_distance` parameter that can exceed 2.\n",
        "\n",
        "# Incorporating Bigrams\n",
        "I retrieve bigrams from the `bigrams.txt` file sourced from Moodle. If a particular bigram is absent from this file, I compute its probability as the product of the probabilities of its constituent words.\n",
        "\n",
        "# Beam Search\n",
        "For each word, the algorithm generates a list of correction candidates. Subsequently, I retain the `beam_search_N` most probable correction variants to correct the sentence (my algorithm processes text on a sentence-by-sentence basis). Then, for each pair consisting of the last word of the current sentence variant and the candidates for the current word, I calculate the probability using the bigram data. After each iteration of beam search, I decide whether to normalize the probabilities, as failure to do so would result in all probabilities diminishing too rapidly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(\"big.txt\").read()\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sent_tokenize(text)]"
      ],
      "metadata": {
        "id": "v2OTE_7iW-11"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_sentences = 100\n",
        "test_tokenized_sentences = random.sample(tokenized_sentences, N_sentences)"
      ],
      "metadata": {
        "id": "gENrh-G_Yg-y"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code implements a class that facilitates the construction of a dataset for evaluating a spell checker. This class takes text, the number of sentences to be sampled from the text, the noise level indicating how much the words will be changed, and the probability of a word being noisy."
      ],
      "metadata": {
        "id": "3c3r1LONyR3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class SpellCheckerTestDatasetBuilder:\n",
        "  def __init__(self, text, N_sentences=1, noise_level=1, noise_p=0.5):\n",
        "    self.text = text\n",
        "    self.tokenized_sentences = random.sample([word_tokenize(sentence) for sentence in sent_tokenize(text)], N_sentences)\n",
        "    self.N_sentences=N_sentences\n",
        "    self.noise_level = noise_level\n",
        "    self.noise_p = noise_p\n",
        "    self.noisy_tokenized_sentences = self.add_noise(self.tokenized_sentences)\n",
        "\n",
        "  def get_noisy_text(self):\n",
        "    ans = ''\n",
        "    for sentence in self.noisy_tokenized_sentences:\n",
        "      ans += ' '.join(sentence) + ' '\n",
        "    return ans.strip()\n",
        "\n",
        "  def add_noise_to_word(self, word):\n",
        "    if not word.isalpha() or random.random() > self.noise_p:\n",
        "      return word\n",
        "    r = random.random()\n",
        "    if 0 < r <= 0.25: # Insert\n",
        "      random_position = random.randint(0, len(word))\n",
        "      random_letter = chr(random.randint(97, 122))\n",
        "      modified_word = word[:random_position] + random_letter + word[random_position:]\n",
        "    if 0.25 < r <= 0.5: # Delete\n",
        "      random_position = random.randint(0, len(word) - 1)\n",
        "      modified_word = word[:random_position] + word[random_position + 1:]\n",
        "    if 0.5 < r <= 0.75: # Replace\n",
        "      random_position = random.randint(0, len(word) - 1)\n",
        "      random_letter = chr(random.randint(97, 122))\n",
        "      modified_word = word[:random_position] + random_letter + word[random_position + 1:]\n",
        "    if 0.75 < r: # Transpose\n",
        "      if len(word) < 2:\n",
        "          return word\n",
        "      random_position = random.randint(0, len(word) - 2)\n",
        "      modified_word = list(word)\n",
        "      modified_word[random_position], modified_word[random_position + 1] = modified_word[random_position + 1], modified_word[random_position]\n",
        "      modified_word = ''.join(modified_word)\n",
        "    return modified_word\n",
        "\n",
        "  def noise_word(self, word):\n",
        "    cur_word = word\n",
        "    for i in range(self.noise_level):\n",
        "      cur_word = self.add_noise_to_word(cur_word)\n",
        "    return cur_word\n",
        "\n",
        "  def add_noise(self, tokenized_sentences):\n",
        "    noisy_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "      noisy_sentence = []\n",
        "      for word in sentence:\n",
        "        noisy_sentence.append(self.noise_word(word))\n",
        "      noisy_sentences.append(noisy_sentence)\n",
        "    return noisy_sentences\n",
        "\n",
        "  def check_accuracy(self, text):\n",
        "    c = 0\n",
        "    N_words = 0\n",
        "    for sent_idx, sentence in enumerate([word_tokenize(sentence) for sentence in sent_tokenize(text)]):\n",
        "      for word_idx, word in enumerate(sentence):\n",
        "        N_words += 1\n",
        "        if sent_idx >=len(self.tokenized_sentences) or word_idx >= len(self.tokenized_sentences[sent_idx]):\n",
        "          continue\n",
        "        if word.lower() == self.tokenized_sentences[sent_idx][word_idx].lower():\n",
        "          c += 1\n",
        "    return c / N_words"
      ],
      "metadata": {
        "id": "18Jh5csBY5IY"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building noisy dataset from 'big.txt' file\n",
        "text = open('some_text_about_nature.txt').read()\n",
        "test_dataset = SpellCheckerTestDatasetBuilder(text=text, N_sentences=10, noise_level=1, noise_p=1)\n",
        "noisy_text = test_dataset.get_noisy_text()"
      ],
      "metadata": {
        "id": "IzwR9fBeawbG"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(noisy_text)"
      ],
      "metadata": {
        "id": "t9EeJ-wVi3G0",
        "outputId": "f767165b-0ba0-492f-8d8b-af21b50ddaad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ehe rapdi increaseq i urbaniuzation as usepd mots orf teh revources liek treesc , mnerals , fodsil feuls , ad ater . Natur refevs t tue inteaction betxeen tje physcial surrouzndings aroundf su anfd tehe lifs wihin i lkie atmospherge , climae , natura resouces , ecosstem , ftlora , faun , nad huymans . Humanq ix thir quets fr ag comfortablx lviing hvae beepn sing hte resouces fo natuer mindlesly . Tqe soke dischargei fom actory untis nd exhauts tancks pof carb ys fcontaminating tht iar thgt woe bpeathe . Rgiht frmm he fdod wb emt , hte clothse ew war , asd te housie ew liv ni gis prvided yb naturte . Th us fo pestidides nad chemiccal lertilizers imn agricnlture adss tzo soi pollutoin . Ii js thj sprimary souurce ovf ell th necesisties bor teh nourishmnt xf ll lving beingsg gon Eahth . eW shouls conserv gwater yb is rationpal se , raipwater harvesing , chnecking tce sureace outfolow , ec . Ngature ii sacid o me tqe greatet teache a iyt tecahes hte lbessons f immogrtality an mortaliyt . he entmre cyle os reupiration s reeulated yb natured .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splchk = SpellChecker(WORDS_FREQS=WORDS, BIGRAMS=BIGRAMS, edit_distance=2, beam_search_N=100)\n",
        "norvigsp = NorvigSpellChecker()\n",
        "corrected_text = splchk.text_correction(noisy_text)\n",
        "corrected_text_norvig = norvigsp.text_correction(noisy_text)\n",
        "print(f\"My model: {test_dataset.check_accuracy(corrected_text)*100: .2f}% of text words corrected properly\")\n",
        "print(f\"Norvig's model: {test_dataset.check_accuracy(corrected_text_norvig)*100: .2f}% of text words corrected properly\")"
      ],
      "metadata": {
        "id": "hdg_wiu-hROU",
        "outputId": "216d5633-777d-43e1-f997-84028331fdfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My model:  71.42% of text words corrected properly\n",
            "Norvig's model:  67.53% of text words corrected properly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models context understanding"
      ],
      "metadata": {
        "id": "sh0yS7yt30tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splchk = SpellChecker(WORDS_FREQS=WORDS, BIGRAMS=BIGRAMS, edit_distance=2, beam_search_N=10)\n",
        "norvigsp = NorvigSpellChecker()\n",
        "first_text = \"We sing ckngs!\"\n",
        "second_text = \"We put on ckngs!\"\n",
        "print(\"Misspelled: \"+first_text+\"  | Corrected by my model:\", splchk.text_correction(first_text), \"Corrected by Norvig's model:\", norvigsp.text_correction(first_text))\n",
        "print(\"Misspelled: \"+second_text+\"| Corrected by my model:\", splchk.text_correction(second_text), \"Corrected by Norvig's model:\", norvigsp.text_correction(second_text))"
      ],
      "metadata": {
        "id": "uzdA5qPc3wx7",
        "outputId": "d6f272be-9ba7-4e73-a3c3-11cdb2ffdf2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misspelled: We sing ckngs!  | Corrected by my model: we sing songs ! Corrected by Norvig's model: we sing kings ! \n",
            "Misspelled: We put on ckngs!| Corrected by my model: we put on rings ! Corrected by Norvig's model: we put on kings ! \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}